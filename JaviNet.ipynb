{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "going-constitutional",
   "metadata": {},
   "source": [
    "# Avance RecSys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-hughes",
   "metadata": {},
   "source": [
    "Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-journey",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pkg_resources import packaging\n",
    "from PIL import Image\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-judges",
   "metadata": {},
   "source": [
    "## CLIP\n",
    "\n",
    "Importar librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "reserved-hungary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.13.0+cu117\n"
     ]
    }
   ],
   "source": [
    "import Clip.clip.clip as clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "descending-galaxy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "statistical-trouble",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "model.cuda().eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-nicaragua",
   "metadata": {},
   "source": [
    "Calcular embeddings para el set de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "considered-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_image(path_img, text_inputs, show=False, top=True):\n",
    "    im = Image.open(path_img)\n",
    "    if show:\n",
    "        im.show()\n",
    "    image_input = preprocess(im).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input)\n",
    "        text_features = model.encode_text(text_inputs)\n",
    "    \n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    if top:\n",
    "        value, index = similarity[0].topk(1)\n",
    "        return image_features, text_descriptors[index]\n",
    "    else:\n",
    "        values, indices = similarity[0].topk(5)\n",
    "        \n",
    "        print(\"\\nTop predictions:\\n\")\n",
    "        for value, index in zip(values, indices):\n",
    "            print(f\"{text_descriptors[index]:>16s}: {100 * value.item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "simplified-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test():\n",
    "    with open(\"classes.txt\", \"r\") as file:\n",
    "        tmp = file.readlines()\n",
    "        text_descriptors = [line.strip() for line in tmp]\n",
    "\n",
    "    text_inputs = torch.cat([clip.tokenize(c) for c in text_descriptors]).to(device)\n",
    "    \n",
    "    with open(\"images_test.txt\", \"r\") as file:\n",
    "        tmp = file.readlines()\n",
    "        images = [line.strip() for line in tmp] \n",
    "    \n",
    "    embeddings = []\n",
    "    for image in images:\n",
    "        path = f\"iui_wikimedia_images/{image}\"\n",
    "        embedding, result = evaluate_image(path, text_inputs)\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "nonprofit-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = evaluate_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-chain",
   "metadata": {},
   "source": [
    "## CuratorNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-subscriber",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
